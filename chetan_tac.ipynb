{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3aa5e5e6-fc0b-4bdb-922c-26c5f38af87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69ba80e5-9b44-4343-8258-23bf3c70a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ac2a4c-78df-42c8-a4c5-6e4060c4f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the complete analysis\n",
    "    \"\"\"\n",
    "    print(\"Loading data files...\")\n",
    "    # Load the CSV files\n",
    "    transactions_df = pd.read_csv('TRANSACTION_TAKEHOME.csv')\n",
    "    products_df = pd.read_csv('PRODUCTS_TAKEHOME.csv')\n",
    "    users_df = pd.read_csv('USER_TAKEHOME.csv')\n",
    "    \n",
    "    print(\"\\n===== PART 1: DATA EXPLORATION =====\")\n",
    "    # Explore and clean the data\n",
    "    clean_transactions, clean_products, clean_users = explore_data(transactions_df, products_df, users_df)\n",
    "    \n",
    "    print(\"\\n===== PART 2: ANSWERING SQL QUERIES =====\")\n",
    "    # Answer the closed-ended and open-ended questions\n",
    "    answer_queries(clean_transactions, clean_products, clean_users)\n",
    "    \n",
    "    print(\"\\n===== PART 3: STAKEHOLDER COMMUNICATION =====\")\n",
    "    # Generate stakeholder communication\n",
    "    generate_communication(clean_transactions, clean_products, clean_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7eb8db0-32d0-47b0-8358-2ce3d97193c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(transactions_df, products_df, users_df):\n",
    "    \"\"\"\n",
    "    Explore and clean the datasets, identify data quality issues\n",
    "    \"\"\"\n",
    "    print(\"\\nBasic information about the datasets:\")\n",
    "    print(f\"Transactions: {transactions_df.shape[0]} rows, {transactions_df.shape[1]} columns\")\n",
    "    print(f\"Products: {products_df.shape[0]} rows, {products_df.shape[1]} columns\")\n",
    "    print(f\"Users: {users_df.shape[0]} rows, {users_df.shape[1]} columns\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values in each dataset:\")\n",
    "    print(\"Transactions missing values:\")\n",
    "    print(transactions_df.isnull().sum())\n",
    "    print(\"\\nProducts missing values:\")\n",
    "    print(products_df.isnull().sum())\n",
    "    print(\"\\nUsers missing values:\")\n",
    "    print(users_df.isnull().sum())\n",
    "    \n",
    "    # Data quality check for transactions\n",
    "    print(\"\\nData quality issues in transactions:\")\n",
    "    # Check date formats\n",
    "    print(f\"Purchase date format sample: {transactions_df['PURCHASE_DATE'].head(3).tolist()}\")\n",
    "    print(f\"Scan date format sample: {transactions_df['SCAN_DATE'].head(3).tolist()}\")\n",
    "    \n",
    "    # Check barcode format\n",
    "    print(f\"Barcode format sample: {transactions_df['BARCODE'].head(5).tolist()}\")\n",
    "    \n",
    "    # Data quality check for products\n",
    "    print(\"\\nData quality issues in products:\")\n",
    "    print(f\"Barcode format sample: {products_df['BARCODE'].head(5).tolist()}\")\n",
    "    \n",
    "    # Data quality check for users\n",
    "    print(\"\\nData quality issues in users:\")\n",
    "    print(f\"Date format sample: {users_df['BIRTH_DATE'].head(3).tolist()}, {users_df['CREATED_DATE'].head(3).tolist()}\")\n",
    "    \n",
    "    # Clean datasets\n",
    "    clean_transactions = clean_transaction_data(transactions_df)\n",
    "    clean_products = clean_product_data(products_df)\n",
    "    clean_users = clean_user_data(users_df)\n",
    "    \n",
    "    # Additional data quality check - barcode consistency\n",
    "    print(\"\\nBarcode consistency check:\")\n",
    "    trans_barcodes = set(clean_transactions['BARCODE'].dropna().unique())\n",
    "    prod_barcodes = set(clean_products['BARCODE'].dropna().unique())\n",
    "    print(f\"Barcodes in transactions: {len(trans_barcodes)}\")\n",
    "    print(f\"Barcodes in products: {len(prod_barcodes)}\")\n",
    "    print(f\"Barcodes in transactions but not in products: {len(trans_barcodes - prod_barcodes)}\")\n",
    "    \n",
    "    return clean_transactions, clean_products, clean_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f169fb6-a1a1-4335-a377-6e872ac47f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_transaction_data(df):\n",
    "    \"\"\"\n",
    "    Clean the transaction dataset\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # Convert dates to datetime format (remove timezone if present)\n",
    "    clean_df['PURCHASE_DATE'] = pd.to_datetime(clean_df['PURCHASE_DATE'], format='%d-%m-%Y', errors='coerce')\n",
    "    clean_df['SCAN_DATE'] = pd.to_datetime(clean_df['SCAN_DATE'], errors='coerce')\n",
    "    \n",
    "    # Remove timezone info to avoid timezone-aware vs timezone-naive issues\n",
    "    if hasattr(clean_df['SCAN_DATE'].dt, 'tz_localize'):\n",
    "        clean_df['SCAN_DATE'] = clean_df['SCAN_DATE'].dt.tz_localize(None)\n",
    "    \n",
    "    # Fix BARCODE format\n",
    "    clean_df['BARCODE'] = clean_df['BARCODE'].astype(str).str.replace('.00', '', regex=False)\n",
    "    clean_df.loc[clean_df['BARCODE'] == '0', 'BARCODE'] = np.nan\n",
    "    clean_df.loc[clean_df['BARCODE'] == 'nan', 'BARCODE'] = np.nan\n",
    "    \n",
    "    # Convert FINAL_SALE to numeric\n",
    "    clean_df['FINAL_SALE'] = pd.to_numeric(clean_df['FINAL_SALE'], errors='coerce')\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db6b6aa5-78ea-41df-a703-af2a748e1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_product_data(df):\n",
    "    \"\"\"\n",
    "    Clean the product dataset\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # Fix BARCODE format (convert scientific notation)\n",
    "    clean_df['BARCODE'] = clean_df['BARCODE'].astype(str)\n",
    "    # Convert scientific notation to regular numbers\n",
    "    clean_df['BARCODE'] = clean_df['BARCODE'].apply(\n",
    "        lambda x: str(int(float(x))) if x and ('E' in x.upper() or 'e' in x) else x)\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55b2cd73-c598-404f-b5b9-5d0f26c75cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_user_data(df):\n",
    "    \"\"\"\n",
    "    Clean the user dataset\n",
    "    \"\"\"\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # Convert dates to datetime format\n",
    "    clean_df['CREATED_DATE'] = pd.to_datetime(clean_df['CREATED_DATE'], errors='coerce')\n",
    "    clean_df['BIRTH_DATE'] = pd.to_datetime(clean_df['BIRTH_DATE'], errors='coerce')\n",
    "    \n",
    "    # Remove timezone info to avoid timezone-aware vs timezone-naive issues\n",
    "    if hasattr(clean_df['CREATED_DATE'].dt, 'tz_localize'):\n",
    "        clean_df['CREATED_DATE'] = clean_df['CREATED_DATE'].dt.tz_localize(None)\n",
    "    if hasattr(clean_df['BIRTH_DATE'].dt, 'tz_localize'):\n",
    "        clean_df['BIRTH_DATE'] = clean_df['BIRTH_DATE'].dt.tz_localize(None)\n",
    "    \n",
    "    # Calculate age\n",
    "    reference_date = pd.to_datetime('2024-08-01')\n",
    "    clean_df['AGE'] = clean_df['BIRTH_DATE'].apply(\n",
    "        lambda x: (reference_date - x).days // 365 if pd.notna(x) else None)\n",
    "    \n",
    "    # Add generation\n",
    "    def assign_generation(birth_date):\n",
    "        if pd.isna(birth_date):\n",
    "            return 'Unknown'\n",
    "        \n",
    "        birth_year = birth_date.year\n",
    "        \n",
    "        if birth_year >= 1997 and birth_year <= 2012:\n",
    "            return 'Gen Z'\n",
    "        elif birth_year >= 1981 and birth_year <= 1996:\n",
    "            return 'Millennial'\n",
    "        elif birth_year >= 1965 and birth_year <= 1980:\n",
    "            return 'Gen X'\n",
    "        elif birth_year >= 1946 and birth_year <= 1964:\n",
    "            return 'Baby Boomer'\n",
    "        elif birth_year < 1946:\n",
    "            return 'Silent Generation'\n",
    "        else:\n",
    "            return 'Unknown'\n",
    "    \n",
    "    clean_df['GENERATION'] = clean_df['BIRTH_DATE'].apply(assign_generation)\n",
    "    \n",
    "    return clean_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a594e605-6331-4e2e-b7ca-22aa866adcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_queries(transactions_df, products_df, users_df):\n",
    "    \"\"\"\n",
    "    Answer the required SQL-like queries\n",
    "    \"\"\"\n",
    "    # Merge dataframes for analysis\n",
    "    merged_df = merge_dataframes(transactions_df, products_df, users_df)\n",
    "    \n",
    "    # Query 1: Top 5 brands by receipts scanned among users 21 and over\n",
    "    print(\"\\nQuery 1: Top 5 brands by receipts scanned among users 21 and over\")\n",
    "    top_brands_receipts = query_top_brands_by_receipts(merged_df)\n",
    "    print(top_brands_receipts)\n",
    "    \n",
    "    # Query 2: Top 5 brands by sales among users with accounts for at least six months\n",
    "    print(\"\\nQuery 2: Top 5 brands by sales among users with accounts for at least six months\")\n",
    "    top_brands_sales = query_top_brands_by_sales(merged_df)\n",
    "    print(top_brands_sales)\n",
    "    \n",
    "    # Query 3: Percentage of sales in Health & Wellness category by generation\n",
    "    print(\"\\nQuery 3: Percentage of sales in Health & Wellness category by generation\")\n",
    "    health_wellness_pct = query_health_wellness_by_generation(merged_df)\n",
    "    print(health_wellness_pct)\n",
    "    \n",
    "    # Open-ended Question 1: Who are Fetch's power users?\n",
    "    print(\"\\nOpen-ended Question 1: Who are Fetch's power users?\")\n",
    "    print(\"Assumption: Power users are defined based on receipt count, total spend, and engagement frequency\")\n",
    "    power_users = query_power_users(merged_df)\n",
    "    print(power_users)\n",
    "    \n",
    "    # Open-ended Question 2: Which is the leading brand in the Dips & Salsa category?\n",
    "    print(\"\\nOpen-ended Question 2: Which is the leading brand in the Dips & Salsa category?\")\n",
    "    print(\"Assumption: Leading brand is determined by total sales volume\")\n",
    "    leading_dips_brand = query_leading_dips_brand(merged_df)\n",
    "    print(leading_dips_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6ed9814-c913-4ab7-b327-a563405f6990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(transactions_df, products_df, users_df):\n",
    "    \"\"\"\n",
    "    Merge the three dataframes for analysis\n",
    "    \"\"\"\n",
    "    # Merge transactions with products\n",
    "    merged_df = transactions_df.merge(products_df, on='BARCODE', how='left')\n",
    "    \n",
    "    # Merge with users\n",
    "    merged_df = merged_df.merge(users_df, left_on='USER_ID', right_on='ID', how='left')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96fa8196-1f93-4df3-af77-8d5b42bfa995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_top_brands_by_receipts(df):\n",
    "    \"\"\"\n",
    "    Top 5 brands by receipts scanned among users 21 and over\n",
    "    \"\"\"\n",
    "    # Filter for users 21 and over\n",
    "    adult_df = df[df['AGE'] >= 21]\n",
    "    \n",
    "    # Group by brand and count unique receipts\n",
    "    result = adult_df.groupby('BRAND')['RECEIPT_ID'].nunique().reset_index()\n",
    "    result.columns = ['BRAND', 'RECEIPT_COUNT']\n",
    "    \n",
    "    # Sort and get top 5\n",
    "    top_brands = result.sort_values('RECEIPT_COUNT', ascending=False).head(5)\n",
    "    \n",
    "    return top_brands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a05ffc8a-50c8-4628-9547-2636e3eb2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_top_brands_by_sales(df):\n",
    "    \"\"\"\n",
    "    Top 5 brands by sales among users that have had their account for at least six months\n",
    "    \"\"\"\n",
    "    # Calculate reference date (6 months ago from latest transaction)\n",
    "    max_date = df['SCAN_DATE'].max()\n",
    "    \n",
    "    # Handle case where max_date might be NaT\n",
    "    if pd.isna(max_date):\n",
    "        reference_date = pd.to_datetime('2024-08-01') - pd.DateOffset(months=6)\n",
    "    else:\n",
    "        reference_date = max_date - pd.DateOffset(months=6)\n",
    "    \n",
    "    # Filter for users with accounts older than 6 months\n",
    "    established_users_df = df[df['CREATED_DATE'] <= reference_date]\n",
    "    \n",
    "    # Group by brand and sum sales\n",
    "    result = established_users_df.groupby('BRAND')['FINAL_SALE'].sum().reset_index()\n",
    "    \n",
    "    # Sort and get top 5\n",
    "    top_brands = result.sort_values('FINAL_SALE', ascending=False).head(5)\n",
    "    \n",
    "    return top_brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6921eb20-f8ce-4de4-a86a-de4751ef2fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_health_wellness_by_generation(df):\n",
    "    \"\"\"\n",
    "    Percentage of sales in Health & Wellness category by generation\n",
    "    \"\"\"\n",
    "    # Calculate total sales by generation\n",
    "    total_sales = df.groupby('GENERATION')['FINAL_SALE'].sum().reset_index()\n",
    "    total_sales.columns = ['GENERATION', 'TOTAL_SALES']\n",
    "    \n",
    "    # Calculate Health & Wellness sales by generation\n",
    "    hw_sales = df[df['CATEGORY_1'] == 'Health & Wellness'].groupby('GENERATION')['FINAL_SALE'].sum().reset_index()\n",
    "    hw_sales.columns = ['GENERATION', 'HW_SALES']\n",
    "    \n",
    "    # Merge and calculate percentage\n",
    "    result = total_sales.merge(hw_sales, on='GENERATION', how='left')\n",
    "    result['HW_SALES'] = result['HW_SALES'].fillna(0)\n",
    "    result['PERCENTAGE'] = (result['HW_SALES'] / result['TOTAL_SALES'] * 100).round(2)\n",
    "    \n",
    "    # Sort by percentage\n",
    "    result = result.sort_values('PERCENTAGE', ascending=False)\n",
    "    \n",
    "    return result[['GENERATION', 'PERCENTAGE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e732c74f-ce78-445b-b678-dd9680f91048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_power_users(df):\n",
    "    \"\"\"\n",
    "    Identify Fetch's power users\n",
    "    \"\"\"\n",
    "    # Calculate metrics by user\n",
    "    \n",
    "    # 1. Receipt count\n",
    "    receipt_count = df.groupby('USER_ID')['RECEIPT_ID'].nunique().reset_index()\n",
    "    receipt_count.columns = ['USER_ID', 'RECEIPT_COUNT']\n",
    "    \n",
    "    # 2. Total spend\n",
    "    total_spend = df.groupby('USER_ID')['FINAL_SALE'].sum().reset_index()\n",
    "    total_spend.columns = ['USER_ID', 'TOTAL_SPEND']\n",
    "    \n",
    "    # 3. Scanning frequency (days between first and last scan divided by number of receipts)\n",
    "    try:\n",
    "        scan_dates = df.groupby('USER_ID')['SCAN_DATE'].agg(['min', 'max', 'count']).reset_index()\n",
    "        scan_dates.columns = ['USER_ID', 'FIRST_SCAN', 'LAST_SCAN', 'SCAN_COUNT']\n",
    "        \n",
    "        # Handle potential NaT values\n",
    "        scan_dates = scan_dates.dropna(subset=['FIRST_SCAN', 'LAST_SCAN'])\n",
    "        \n",
    "        # Calculate days active (handling potential timezone issues)\n",
    "        scan_dates['DAYS_ACTIVE'] = (scan_dates['LAST_SCAN'] - scan_dates['FIRST_SCAN']).dt.total_seconds() / (24*60*60)\n",
    "        scan_dates['FREQUENCY'] = scan_dates['SCAN_COUNT'] / (scan_dates['DAYS_ACTIVE'] + 1)  # Add 1 to avoid division by zero\n",
    "        \n",
    "        # 4. Recency (days since last scan)\n",
    "        max_date = df['SCAN_DATE'].max()\n",
    "        if pd.notna(max_date):\n",
    "            scan_dates['RECENCY'] = (max_date - scan_dates['LAST_SCAN']).dt.total_seconds() / (24*60*60)\n",
    "        else:\n",
    "            scan_dates['RECENCY'] = 0  # Default if no max date\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in frequency calculation: {e}\")\n",
    "        # Create a default dataframe if there's an issue\n",
    "        scan_dates = pd.DataFrame({'USER_ID': receipt_count['USER_ID'], \n",
    "                                  'FREQUENCY': 0,\n",
    "                                  'RECENCY': 0})\n",
    "    \n",
    "    # Combine metrics\n",
    "    user_metrics = receipt_count.merge(total_spend, on='USER_ID', how='left')\n",
    "    user_metrics = user_metrics.merge(scan_dates[['USER_ID', 'FREQUENCY', 'RECENCY']], on='USER_ID', how='left')\n",
    "    \n",
    "    # Fill NaN values\n",
    "    user_metrics = user_metrics.fillna({'TOTAL_SPEND': 0, 'FREQUENCY': 0, 'RECENCY': 0})\n",
    "    \n",
    "    # Normalize metrics\n",
    "    for column in ['RECEIPT_COUNT', 'TOTAL_SPEND', 'FREQUENCY']:\n",
    "        max_val = user_metrics[column].max()\n",
    "        if max_val > 0:  # Avoid division by zero\n",
    "            user_metrics[f'NORM_{column}'] = user_metrics[column] / max_val\n",
    "        else:\n",
    "            user_metrics[f'NORM_{column}'] = 0\n",
    "    \n",
    "    # For recency, lower is better, so invert the normalization\n",
    "    max_recency = user_metrics['RECENCY'].max()\n",
    "    if max_recency > 0:  # Avoid division by zero\n",
    "        user_metrics['NORM_RECENCY'] = 1 - (user_metrics['RECENCY'] / max_recency)\n",
    "    else:\n",
    "        user_metrics['NORM_RECENCY'] = 1  # All users have same recency\n",
    "    \n",
    "    # Calculate power score\n",
    "    user_metrics['POWER_SCORE'] = (\n",
    "        user_metrics['NORM_RECEIPT_COUNT'] * 0.4 +\n",
    "        user_metrics['NORM_TOTAL_SPEND'] * 0.3 +\n",
    "        user_metrics['NORM_FREQUENCY'] * 0.2 +\n",
    "        user_metrics['NORM_RECENCY'] * 0.1\n",
    "    )\n",
    "    \n",
    "    # Get top 10 power users\n",
    "    top_users = user_metrics.sort_values('POWER_SCORE', ascending=False).head(10)\n",
    "    \n",
    "    return top_users[['USER_ID', 'RECEIPT_COUNT', 'TOTAL_SPEND', 'POWER_SCORE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1374ced-1123-4f36-a8d8-5188ead00cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_leading_dips_brand(df):\n",
    "    \"\"\"\n",
    "    Find the leading brand in Dips & Salsa category\n",
    "    \n",
    "    Assumptions:\n",
    "    1. We'll search for 'Dips' and 'Salsa' in category fields\n",
    "    2. Leading brand determined by total sales\n",
    "    \"\"\"\n",
    "    # First check if Dips & Salsa category exists\n",
    "    dips_salsa_df = None\n",
    "    \n",
    "    # Check in different category columns\n",
    "    for cat_col in ['CATEGORY_1', 'CATEGORY_2', 'CATEGORY_3', 'CATEGORY_4']:\n",
    "        if cat_col in df.columns:\n",
    "            # Look for either Dips or Salsa in the category name\n",
    "            matching_rows = df[df[cat_col].str.contains('Dips|Salsa', case=False, na=False)]\n",
    "            if not matching_rows.empty:\n",
    "                if dips_salsa_df is None:\n",
    "                    dips_salsa_df = matching_rows\n",
    "                else:\n",
    "                    dips_salsa_df = pd.concat([dips_salsa_df, matching_rows]).drop_duplicates()\n",
    "    \n",
    "    if dips_salsa_df is not None and not dips_salsa_df.empty:\n",
    "        # Calculate sales by brand\n",
    "        brand_sales = dips_salsa_df.groupby('BRAND')['FINAL_SALE'].sum().reset_index()\n",
    "        \n",
    "        # Filter out null/NaN brands\n",
    "        brand_sales = brand_sales.dropna(subset=['BRAND'])\n",
    "        \n",
    "        if not brand_sales.empty:\n",
    "            # Sort and get top brand\n",
    "            top_brand = brand_sales.sort_values('FINAL_SALE', ascending=False).head(1)\n",
    "            return top_brand\n",
    "    \n",
    "    return \"No 'Dips & Salsa' category found or no sales data available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cd401e6-dd0f-491b-9689-db2a013f5f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data files...\n",
      "\n",
      "===== PART 1: DATA EXPLORATION =====\n",
      "\n",
      "Basic information about the datasets:\n",
      "Transactions: 50000 rows, 8 columns\n",
      "Products: 845552 rows, 7 columns\n",
      "Users: 100000 rows, 6 columns\n",
      "\n",
      "Missing values in each dataset:\n",
      "Transactions missing values:\n",
      "RECEIPT_ID           0\n",
      "PURCHASE_DATE        0\n",
      "SCAN_DATE            0\n",
      "STORE_NAME           0\n",
      "USER_ID              0\n",
      "BARCODE           5762\n",
      "FINAL_QUANTITY       0\n",
      "FINAL_SALE           0\n",
      "dtype: int64\n",
      "\n",
      "Products missing values:\n",
      "CATEGORY_1         111\n",
      "CATEGORY_2        1424\n",
      "CATEGORY_3       60566\n",
      "CATEGORY_4      778093\n",
      "MANUFACTURER    226474\n",
      "BRAND           226472\n",
      "BARCODE           4025\n",
      "dtype: int64\n",
      "\n",
      "Users missing values:\n",
      "ID                  0\n",
      "CREATED_DATE        0\n",
      "BIRTH_DATE       3675\n",
      "STATE            4812\n",
      "LANGUAGE        30508\n",
      "GENDER           5892\n",
      "dtype: int64\n",
      "\n",
      "Data quality issues in transactions:\n",
      "Purchase date format sample: ['2024-08-21', '2024-07-20', '2024-08-18']\n",
      "Scan date format sample: ['2024-08-21 14:19:06.539 Z', '2024-07-20 09:50:24.206 Z', '2024-08-19 15:38:56.813 Z']\n",
      "Barcode format sample: [15300014978.0, nan, 78742229751.0, 783000000000.0, 47900501183.0]\n",
      "\n",
      "Data quality issues in products:\n",
      "Barcode format sample: [796494407820.0, 23278011028.0, 461817824225.0, 35000466815.0, 806810850459.0]\n",
      "\n",
      "Data quality issues in users:\n",
      "Date format sample: ['2000-08-11 00:00:00.000 Z', '2001-09-24 04:00:00.000 Z', '1994-10-28 00:00:00.000 Z'], ['2020-06-24 20:17:54.000 Z', '2021-01-03 19:53:55.000 Z', '2023-05-31 18:42:18.000 Z']\n",
      "\n",
      "Barcode consistency check:\n",
      "Barcodes in transactions: 8471\n",
      "Barcodes in products: 841343\n",
      "Barcodes in transactions but not in products: 4047\n",
      "\n",
      "===== PART 2: ANSWERING SQL QUERIES =====\n",
      "\n",
      "Query 1: Top 5 brands by receipts scanned among users 21 and over\n",
      "          BRAND  RECEIPT_COUNT\n",
      "10         DOVE              3\n",
      "26  NERDS CANDY              3\n",
      "14  GREAT VALUE              2\n",
      "46      TRIDENT              2\n",
      "0        ALWAYS              1\n",
      "\n",
      "Query 2: Top 5 brands by sales among users with accounts for at least six months\n",
      "          BRAND  FINAL_SALE\n",
      "5           CVS       72.00\n",
      "40      TRIDENT       46.72\n",
      "9          DOVE       42.88\n",
      "3   COORS LIGHT       34.96\n",
      "39     TRESEMMÃ‰       14.58\n",
      "\n",
      "Query 3: Percentage of sales in Health & Wellness category by generation\n",
      "          GENERATION  PERCENTAGE\n",
      "0        Baby Boomer       26.69\n",
      "1              Gen X       15.71\n",
      "3         Millennial       14.18\n",
      "2              Gen Z        0.00\n",
      "4  Silent Generation        0.00\n",
      "5            Unknown        0.00\n",
      "\n",
      "Open-ended Question 1: Who are Fetch's power users?\n",
      "Assumption: Power users are defined based on receipt count, total spend, and engagement frequency\n",
      "                        USER_ID  RECEIPT_COUNT  TOTAL_SPEND  POWER_SCORE\n",
      "14620  64e62de5ca929250373e6cf5             10        57.65         0.52\n",
      "9149   62925c1be942f00613f7365e             10        49.87         0.48\n",
      "13095  64063c8880552327897186a5              9        43.72         0.48\n",
      "10394  630789e1101ae272a4852287              1       925.64         0.40\n",
      "5445   609af341659cf474018831fb              7        25.55         0.39\n",
      "8551   624dca0770c07012cd5e6c03              7        21.91         0.38\n",
      "10752  6327a07aca87b39d76e03864              7        15.64         0.38\n",
      "5127   605a982894a5c74ba439e5ab              6        56.52         0.36\n",
      "4996   604278958fe03212b47e657b              7        46.61         0.36\n",
      "3097   5eb59d6be7012d13941af5e2              6        45.93         0.35\n",
      "\n",
      "Open-ended Question 2: Which is the leading brand in the Dips & Salsa category?\n",
      "Assumption: Leading brand is determined by total sales volume\n",
      "       BRAND  FINAL_SALE\n",
      "35  TOSTITOS      239.54\n",
      "\n",
      "===== PART 3: STAKEHOLDER COMMUNICATION =====\n",
      "\n",
      "Subject: Fetch Receipt Data Analysis - Key Findings & Next Steps\n",
      "\n",
      "Hi Team,\n",
      "\n",
      "I've completed the initial analysis of the transaction, product, and user datasets. Below are my key findings and questions for further discussion.\n",
      "\n",
      "DATA QUALITY ISSUES & QUESTIONS:\n",
      "1. Inconsistent barcode formatting: Some barcodes include decimal points (.00) while others are stored in scientific notation. This required standardization for proper matching between datasets.\n",
      "\n",
      "2. Missing values: We have significant gaps in product data, with some products missing brand and manufacturer information.\n",
      "\n",
      "3. Date format inconsistencies: Purchase dates and scan dates use different formats, which could lead to analysis errors if not properly converted.\n",
      "\n",
      "4. Mismatched barcodes: Several barcodes in the transaction dataset don't exist in the product dataset, making it difficult to categorize these transactions accurately.\n",
      "\n",
      "INTERESTING TREND:\n",
      "Our analysis shows that Health & Wellness products have significantly different purchase patterns across generations. This suggests an opportunity to tailor our offerings and promotions for specific demographic segments.\n",
      "\n",
      "ACTION ITEMS & QUESTIONS:\n",
      "1. Could we establish a standard barcode format across our systems to ensure consistency?\n",
      "\n",
      "2. I'd like to understand the business logic behind cases where FINAL_QUANTITY is 0 but FINAL_SALE has a value.\n",
      "\n",
      "3. Are there additional product category data that could provide more granularity for our analysis?\n",
      "\n",
      "4. It would be helpful to understand how \"power users\" are currently defined by the business, so we can align our analytics approach accordingly.\n",
      "\n",
      "I'm available to discuss these findings in more detail. Let me know if you'd like to schedule a meeting or if you need any specific parts of the analysis expanded upon.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_communication(transactions_df, products_df, users_df):\n",
    "    \"\"\"\n",
    "    Generate stakeholder communication\n",
    "    \"\"\"\n",
    "    email = \"\"\"\n",
    "Subject: Fetch Receipt Data Analysis - Key Findings & Next Steps\n",
    "\n",
    "Hi Team,\n",
    "\n",
    "I've completed the initial analysis of the transaction, product, and user datasets. Below are my key findings and questions for further discussion.\n",
    "\n",
    "DATA QUALITY ISSUES & QUESTIONS:\n",
    "1. Inconsistent barcode formatting: Some barcodes include decimal points (.00) while others are stored in scientific notation. This required standardization for proper matching between datasets.\n",
    "\n",
    "2. Missing values: We have significant gaps in product data, with some products missing brand and manufacturer information.\n",
    "\n",
    "3. Date format inconsistencies: Purchase dates and scan dates use different formats, which could lead to analysis errors if not properly converted.\n",
    "\n",
    "4. Mismatched barcodes: Several barcodes in the transaction dataset don't exist in the product dataset, making it difficult to categorize these transactions accurately.\n",
    "\n",
    "INTERESTING TREND:\n",
    "Our analysis shows that Health & Wellness products have significantly different purchase patterns across generations. This suggests an opportunity to tailor our offerings and promotions for specific demographic segments.\n",
    "\n",
    "ACTION ITEMS & QUESTIONS:\n",
    "1. Could we establish a standard barcode format across our systems to ensure consistency?\n",
    "\n",
    "2. I'd like to understand the business logic behind cases where FINAL_QUANTITY is 0 but FINAL_SALE has a value.\n",
    "\n",
    "3. Are there additional product category data that could provide more granularity for our analysis?\n",
    "\n",
    "4. It would be helpful to understand how \"power users\" are currently defined by the business, so we can align our analytics approach accordingly.\n",
    "\n",
    "I'm available to discuss these findings in more detail. Let me know if you'd like to schedule a meeting or if you need any specific parts of the analysis expanded upon.\n",
    "\n",
    "Best regards,\n",
    "[Your Name]\n",
    "\"\"\"\n",
    "    print(email)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
